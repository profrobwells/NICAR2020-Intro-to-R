---
title: 'NICAR 2019 - R1: Intro to R and RStudio'
author: "Meghan Hoyer"
date: "3/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Welcome to R

R is a powerful open-source programming and statistical language. Scientists, statisticians and, increasingly, journalists are using it to find answers in mountains of data. People turn to R for everything from analyzing multi-year medical studies to preparing charts and drawing maps. The language itself, known as “base R”, can do quite a bit. But computer scientists have vastly expanded its capabilities with more than 12,000 add-on tools or packages.

R is going to help you do more complex analyses in an easier way. But there is something of a learning curve. For one, R is a strict copy-editor - capitalization and spelling matter, so make sure you stick to some naming conventions and spell things correctly. Beyond that, and the biggest issue is there are approximately 1,543 ways to solve any problem in R. I'd recommend hewing as closely as you can to the principles of "tidy" data as you work. 

The most popular of R packages is a suite of about 20 tools known as the tidyverse. It was developed largely by Hadley Wickham, chief scientist at RStudio, the graphical user interface for R. The tidyverse includes tools for importing, manipulating, editing and visualizing data and is based around a basic principle: that data be tidy, with one observation, or data point, in each row.
"When your data is tidy, each column is a variable, and each row is an observation."

If you're looking for more resources or next steps to help you learn, [here](https://github.com/roncampbell/NICAR2019/blob/master/R%20reading%20list.md) is a pretty comprehensive list of RStudio + R resources, including books, tipsheets and webinars compiled by Ron Campbell and me. 

### First up: The basics of RStudio

RStudio is pretty nifty because it lets you see all the pieces of your project, and it even saves you from mistakes or problems later on by allowing you to run all or part of your code, and by keeping your history and showing you your loaded dataframes, functions and lists.

Here's a primer:

* The CONSOLE (bottom left) is where you can type in commands

* The SOURCE (top left) is where you'll see/open your scripts and also View() any datatable. Datatables opened in source (either by double-clicking from Environment or calling View() in your script or console) can also be explored through sorting and filtering

* ENVIRONMENT (top right) is your loaded data

* HISTORY (top right) is every command you've run, starting with your most recent command

* FILES (bottom left) allows you to navigate through your directory

* PLOTS (bottom left) shows you any output from ggplot

* PACKAGES (bottom left) shows you your system library, your package versions, and allows you to install new packages if needed

* HELP (bottom left) includes documentation for packages

### Setting Up Your Work
Since R is open-source, it is free. You can download R and most packages at CRAN, the Comprehensive R Archive Network. Installing and loading packages is a two-step process. To install the tidyverse, for example, you would enter the following commands at the console - note the quotes around the package name:

```{tidyverse}
install.packages("tidyverse")
```

IRE-NICAR staff has already installed R, R Studio and the tidyverse on the computers we’ll be using in this class. But whenever you start R, you must do step 2: loading the packages you need in memory. All library calls should be included at the top of your scripts; that way, anyone using your code can see and load all the necessary packages to run your code.

So - let's open a new script (File -> New File -> R Script) and load the tidyverse package so we can get to work. Note for library calls there are no quotes around the library name.

```{library}
library(tidyverse)
```

Generally, when you work in R, it's best to create a 'Project' in RStudio to make sure your installed packages, files and all other pieces are in one place. While we aren’t creating a project for this class, we will go partway – we’ll set a working directory. The directory we’ll use is wherever the IRE-NICAR staff placed our data. To set the working directory go to the Session menu and click on Set Working Directory. We’ll then look for the correct directory. Best practice: never code a hard-path to setwd() into your script - it'll work on your machine, but won't on anyone else's.

```{library}
getwd()
setwd()
```

### R Syntax

There are many parts of R that look a lot like other data tools -- functions such as filtering, summing, and the like. But there are two main tools in writing R code that may be less familiar to you. First, to assign a value to a variable, we use an arrow-like thing called an assignment operator. 

Here it is: <- 

We’ll also be writing multi-line formulas, joining each line with a device called a pipe. It looks like this: %>%. There’s a shortcut for this: In Windows, type Shift-Control-M; in Mac, type Shift-Command-M.  The pipe always goes at the end of a line, followed by a return.

Beyond that -- give future you and anyone else who will use or read your code a break: Good R practice is to put spaces around assignment operators, to do a hard return after a pipe, and stick to a naming convention for dataframes, functions and variables. There are some styleguides for R [here](https://style.tidyverse.org/_main.pdf) and [here](https://google.github.io/styleguide/Rguide.xml) that will help you think about best practices.

Also don't forget to comment out your code! You can add a comment by putting in an R Script by putting a '#' before your words.

##Reading in Data

Finally -- let's dig in! Today we’re going to be creating several tables, known in R as data frames, and naming them as we go. 

Here's the [file](https://tinyurl.com/nicar19rstudiodata) you'll need. Download it and put it in the folder for this class that you've set as your directory above.

Now let's use read_csv to pull data from the Census into R.

There are other options for reading in files that are tab or pipe-delimited, in .txt, .xlsx or other formats -- including SAS and Stata files, tables from a SQL database, Google sheets or the like. Note that for some of these, you'll need to download additional libraries. 

```{r read_in_data}
census <- read_csv("ACS_17_5YR_B05002-cities.csv")

```

R does a pretty decent job of guessing your data types, but every once in awhile it messes them up and you'll have to rework them. For instance, check that Id2 value. You can declare specific data types for each column while using read_csv, or you can change types on individual columns. For this, let's just fix the column individually by doing:

```{r rework}
census$Id2 <- as.character(census$Id2)
```

### Looking at your Data
Now, let's open this dataframe up and explore as we might explore an Excel spreadsheet.

In your console, type:

```{viewing}
View(census)
```

To limit the View on a large file you can also call head with a certain number of rows, similar to LIMIT in SQL:

```{view2}
head(census, 20) %>% View()
```

You can sort and filter this View, which allows you to get a basic familiarity with your dataframe. 

But now let's start really digging into this to get an idea of what might be in our data. Summary -- which is part of base R -- isn't a bad place to start looking at things, particularly if you have a lot of numbers, because it returns minimum and maximum values for each column, as well as averages and medians. 

```{summary}
summary(census)
```

### Tidying your data 

OK, that's great and all, but it's still really hard to understand what we're looking at. This data isn't tidy! There are lots of different demographic group breakdowns per row -- we don't have just one observation in each row.

Let's tidy things up using gather. This is a somewhat complex operation, so you don't need to totally understand how it works right now, but basically we're reshaping our data, so that the basic information (FIPS codes, city name and total population) are attached to each individual observation (the breakdowns of the type of population and the count of that group).  We're creating an individual row based on columns 5 - 30 of our dataframe. 

Where data in Excel is usually wide, this makes our data long. The opposite of gather is spread, and it can work to make tables wide, with multiple observations per row if you so desire. Ron Campbell's R3 class will cover gather and spread in more detail.

```{r tidydata}
citypop <- census %>% gather("type", "Population" = (5:30))
```

Open that file up with View(citypop). You'll see that now your datafram has a LOT more rows -- 1,066 of them to be exact -- but a lot fewer columns (only six). Now that things are tidy, we can really start to explore.

### Sanity-checking

You want to make sure the data is OK and you don't have duplicates. So let's look at the number of unique Geographies in this 1,066 row datatable.  The number should match the rows in your original census dataframe.

```{r count1}
citypop %>% distinct(Geography) %>% count()
```

You can also use count to see how many of a certain thing meets your filter criteria. Here, let's see how many cities have a total population above 100,000:

```{r count2}
citypop %>% filter(Total>100000) %>% distinct(Geography) %>% count()
```
### Filtering and arranging your data

You're probably familiar with filtering from Excel, or from using "WHERE" statements in SQL. This is similar.

Let's find the populations born inside California by city. Note that filtering on strings and characters requires a double equal sign.

```{r filtering}
citypop %>% 
  filter(type == "Native_Born_in_state_of_residence") 
```

Ugh! The county total is also in there. That throws everything off.

Let's get rid of that in the same filter by using '!=', which means 'not equal.'  Filtering also takes numerical operators such as < or >.

```{r filtering2}
citypop %>% 
  filter(type == "Native_Born_in_state_of_residence", Id2 != "6059") 
```

This is interesting, but it's hard to do all this math in my head. Let's go back and calculate what percentage each population group is of the whole. 

### Mutating data

In R, creating a new column in your data is called _Mutating_ -- basically, you're changing your dataframe by adding a new column without changing anything in your existing dataframe. 

```{r mutating}
citypop <- citypop %>% 
  mutate(pct = value/Total)
```

If you'd like you can take a look again in View(citypop). Much better. Now we can see what share each group is of the whole.

Let's go back to that filter statement now. And since we don't want to have to go in and sort every time, so let's make it so the smallest share of California-born residents is on top.

```{r filterarrange}
citypop %>% 
  filter(type == "Native_Born_in_state_of_residence", Id2 != "6059") %>% 
  arrange(pct) 
```

## Summarize

Inside the summarize (or summarise) function there are many basic math operators -- including mean, median, minimum, maximum and sum. Let's find out what the typical Orange County city's native-born population looks like, as a share of the total population, and find the maximum and minimum among the cities.

```{r summarise}
citypop %>% 
  filter(type == "Native_Born_in_state_of_residence", Id2 != "6059") %>% 
  summarise(Median = median(pct), Minimum = min(pct), Maximum = max(pct))
```

## Filtering and Summing exercise

So we're working on a story and wanting to send reporters out to cities in Orange County where there are a high concentration of residents who were born in Asian countries. It doesn't matter to us whether they are citizens or not for this story, we're  just looking at the total foreign born population from Asia. 

How would we get to that data?  

First we'll need to filter to isolate the right population groups. Then we'll need to tell R to group by the geography, and finally we'll have to add up citizen and non-citizen counts to get at the total Asian-born population.

There's a lot going on here, so let's break it down:

First, the filter.  If you want to combine any group of things in R, you put brackets around them and delineate them with a 'c' -- so here, we're saying let's filter by choosing these two types of populations. 

You could also write _filter(type == "Foreign_born_Naturalized_U.S._citizen__Asia", type == "Foreign_born_Not_a_U.S._citizen__Asia")_ but that involves more typing, so why?

Next: Similar to how we'd GROUP BY in SQL, we're telling R to group all our findings by each individual city.

Then: We're telling R to do math with Summarize(). So we're telling R to add up the percentage of Asian citizens and non-citizens for each city, and to put those numbers in columns we're calling 'tot_pct' and 'pop_sum'

Finally: We're arranging our data by that 'tot_pct' column, with the largest value on top, so we immediately know where to send our reporters.

#### The code:
```{r exercise}
citypop %>%
  filter(type == 
           c("Foreign_born_Naturalized_U.S._citizen__Asia", 
             "Foreign_born_Not_a_U.S._citizen__Asia")) %>% 
  group_by(Geography) %>%
  summarize(tot_pct = sum(pct), 
            pop_sum = sum(value)) %>% 
  arrange(-tot_pct)
```

